{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_id_x', 'SUBJECT_ID', 'HADM_ID', 'seq_num_x', 'ICD9_CODE', 'ROW_ID',\n",
      "       'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION',\n",
      "       'CGID', 'TEXT', 'TEXT_LENGTH', 'CLEAN_TEXT', 'icd9_code_x',\n",
      "       'long_title', 'row_id_y', 'seq_num_y', 'icd9_code_y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id_x</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>seq_num_x</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TEXT_LENGTH</th>\n",
       "      <th>CLEAN_TEXT</th>\n",
       "      <th>icd9_code_x</th>\n",
       "      <th>long_title</th>\n",
       "      <th>row_id_y</th>\n",
       "      <th>seq_num_y</th>\n",
       "      <th>icd9_code_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "      <td>1394273</td>\n",
       "      <td>2164-10-25</td>\n",
       "      <td>2164-10-25 07:16:00</td>\n",
       "      <td>2164-10-25 07:23:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19150</td>\n",
       "      <td>NPN 1900-0700\\nPt awaiting transfer to floor w...</td>\n",
       "      <td>493</td>\n",
       "      <td>npn pt awaiting transfer floor floor bed becom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47335</td>\n",
       "      <td>1</td>\n",
       "      <td>9749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "      <td>1394273</td>\n",
       "      <td>2164-10-25</td>\n",
       "      <td>2164-10-25 07:16:00</td>\n",
       "      <td>2164-10-25 07:23:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19150</td>\n",
       "      <td>NPN 1900-0700\\nPt awaiting transfer to floor w...</td>\n",
       "      <td>493</td>\n",
       "      <td>npn pt awaiting transfer floor floor bed becom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47336</td>\n",
       "      <td>2</td>\n",
       "      <td>5491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "      <td>1394273</td>\n",
       "      <td>2164-10-25</td>\n",
       "      <td>2164-10-25 07:16:00</td>\n",
       "      <td>2164-10-25 07:23:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19150</td>\n",
       "      <td>NPN 1900-0700\\nPt awaiting transfer to floor w...</td>\n",
       "      <td>493</td>\n",
       "      <td>npn pt awaiting transfer floor floor bed becom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47337</td>\n",
       "      <td>3</td>\n",
       "      <td>3895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "      <td>1394273</td>\n",
       "      <td>2164-10-25</td>\n",
       "      <td>2164-10-25 07:16:00</td>\n",
       "      <td>2164-10-25 07:23:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19150</td>\n",
       "      <td>NPN 1900-0700\\nPt awaiting transfer to floor w...</td>\n",
       "      <td>493</td>\n",
       "      <td>npn pt awaiting transfer floor floor bed becom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47338</td>\n",
       "      <td>4</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112344</td>\n",
       "      <td>10006</td>\n",
       "      <td>142345</td>\n",
       "      <td>1</td>\n",
       "      <td>99591</td>\n",
       "      <td>1394273</td>\n",
       "      <td>2164-10-25</td>\n",
       "      <td>2164-10-25 07:16:00</td>\n",
       "      <td>2164-10-25 07:23:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19150</td>\n",
       "      <td>NPN 1900-0700\\nPt awaiting transfer to floor w...</td>\n",
       "      <td>493</td>\n",
       "      <td>npn pt awaiting transfer floor floor bed becom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47339</td>\n",
       "      <td>5</td>\n",
       "      <td>3893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id_x SUBJECT_ID HADM_ID  seq_num_x ICD9_CODE   ROW_ID  CHARTDATE  \\\n",
       "0    112344      10006  142345          1     99591  1394273 2164-10-25   \n",
       "1    112344      10006  142345          1     99591  1394273 2164-10-25   \n",
       "2    112344      10006  142345          1     99591  1394273 2164-10-25   \n",
       "3    112344      10006  142345          1     99591  1394273 2164-10-25   \n",
       "4    112344      10006  142345          1     99591  1394273 2164-10-25   \n",
       "\n",
       "            CHARTTIME           STORETIME       CATEGORY DESCRIPTION   CGID  \\\n",
       "0 2164-10-25 07:16:00 2164-10-25 07:23:00  Nursing/other      Report  19150   \n",
       "1 2164-10-25 07:16:00 2164-10-25 07:23:00  Nursing/other      Report  19150   \n",
       "2 2164-10-25 07:16:00 2164-10-25 07:23:00  Nursing/other      Report  19150   \n",
       "3 2164-10-25 07:16:00 2164-10-25 07:23:00  Nursing/other      Report  19150   \n",
       "4 2164-10-25 07:16:00 2164-10-25 07:23:00  Nursing/other      Report  19150   \n",
       "\n",
       "                                                TEXT  TEXT_LENGTH  \\\n",
       "0  NPN 1900-0700\\nPt awaiting transfer to floor w...          493   \n",
       "1  NPN 1900-0700\\nPt awaiting transfer to floor w...          493   \n",
       "2  NPN 1900-0700\\nPt awaiting transfer to floor w...          493   \n",
       "3  NPN 1900-0700\\nPt awaiting transfer to floor w...          493   \n",
       "4  NPN 1900-0700\\nPt awaiting transfer to floor w...          493   \n",
       "\n",
       "                                          CLEAN_TEXT icd9_code_x long_title  \\\n",
       "0  npn pt awaiting transfer floor floor bed becom...         NaN        NaN   \n",
       "1  npn pt awaiting transfer floor floor bed becom...         NaN        NaN   \n",
       "2  npn pt awaiting transfer floor floor bed becom...         NaN        NaN   \n",
       "3  npn pt awaiting transfer floor floor bed becom...         NaN        NaN   \n",
       "4  npn pt awaiting transfer floor floor bed becom...         NaN        NaN   \n",
       "\n",
       "   row_id_y  seq_num_y  icd9_code_y  \n",
       "0     47335          1         9749  \n",
       "1     47336          2         5491  \n",
       "2     47337          3         3895  \n",
       "3     47338          4         3995  \n",
       "4     47339          5         3893  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved pickle file\n",
    "final_merged_data = pd.read_pickle('final_merged_data.pkl')\n",
    "# Inspect the data\n",
    "print(final_merged_data.columns)\n",
    "\n",
    "final_merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length=128,           # Maximum length for truncation\n",
    "        truncation=True,\n",
    "        padding='max_length',     # Pad to max length\n",
    "        return_attention_mask=True,  # Create attention masks\n",
    "        return_tensors='pt'       # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Apply tokenizer to the CLEAN_TEXT column\n",
    "texts = final_merged_data['CLEAN_TEXT'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_inputs = [tokenize_function(text) for text in texts]\n",
    "\n",
    "# Extract input_ids and attention_masks for model input\n",
    "input_ids = torch.cat([x['input_ids'] for x in tokenized_inputs], dim=0)\n",
    "attention_masks = torch.cat([x['attention_mask'] for x in tokenized_inputs], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels (ICD9_CODE)\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(final_merged_data['ICD9_CODE'])\n",
    "\n",
    "# Convert to torch tensor\n",
    "labels = torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mjoth\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load pre-trained BERT with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_),  # Number of classes in your labels\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "# Training the model\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Convert labels to Long type\n",
    "        batch_labels = batch_labels.long()\n",
    "\n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_train_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get predictions\n",
    "        predictions.append(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.append(batch_labels.cpu().numpy())\n",
    "\n",
    "# Flatten the predictions and true labels\n",
    "predictions = [item for sublist in predictions for item in sublist]\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Evaluate with accuracy, precision, recall, F1-score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('path_to_save_model')\n",
    "tokenizer.save_pretrained('path_to_save_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained BERT for feature extraction\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.to(device)\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        outputs = bert_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Average of token embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
